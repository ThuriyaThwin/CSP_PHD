= Selecting computations in sequential decision problems =

We look at problems in which a sequence of computational actions
must be performed to achieve a goal, and outcomes of earlier
actions affect selection of later actions. Examples of such
problems are parameter optimization, multi-armed bandits, and
combinatorial games. We introduce an approach to selecting
computations based on value of information and time cost of a
computation, and demonstrate application of the approach to
different domains. The work is based on the theory of rational
metareasoning.

== Introduction ==

In this talk, we will look at sequential decision problems. 
In a broad sense, a sequential decision problem is a problem
for which a solution is a contingency plan - a potentially
indefinite sequence of actions, such that later actions
depend on the outcomes of earlier actions. Some squential decision
problems can be modelled using Markov decision proceses (or,
in a more general setting, partially observable markov decision
processes).

Let us look at a few examples:

  - the sailing domain: (Here comes a description of the sailing
    domain, with wind directions and outcomes of the actions).

  - Multi-armed bandits, in various settings (Here comes the definition
    of the multi-armed bandits, and kinds of questions that can be asked
    about multi-armed bantis).

  - Algorithm parameter tuning - an algorithm must be tuned
    on a training set for best results. Stochastic search, kernel-based
    classifier, parameterized heuristics in routing algorithms.

A class of problems that requires particular attention is two-player combinatorial games. In adversarial search, the next state in which
the agent has to make a decision depends on the action performed by 
the adversary. This class of problems is special because state transition
probabilities depend on the policy.

In many such problems, finding the optimal (or even good enough)
contingency policy in advance is infeasible, and the algorithm performs
computations in every state before committing to a base-level action.
The time spent on deliberation, and the kind of computations affect
the quality of the policy:

  - on the one hand, with more deliberation, a `better' (informally for now)
  base-level action can be chosen.
  - on the other hand, the time spent deliberating may have negative effect
  on the quality of the final solution.

As a simple case, let us look at a path-finding algorith, such as heuristic best-first search (A*, IDA* and similar algorithms). The search is driven
by heuristics which predict the distance to the goal. An advantage of using
a more informative heuristic is in decreasing the search time by pruning more
nodes that do not belong to a more optimal solution. But computing the heuristic takes time, and it may happen so that a more informative heuristic
results in a slower algorithm.

In more complicated settings the deliberation time is significant because the
state is dynamic, changing with time even if the agent does not perform an action. Consider a situation in which a public transport must be chosen,
either the bus or the train. The agent knows the frequences of both means of transportation, and the time of departure of the next train. Should the agent check the schedule of buses in hope that the bus departs and arrives earlier, at the risk of missing the nearest train, or should it commit to going by train, and probably miss the opportunity to arrive earlier by bus?

To answer such questions, we should suggest a way of measuring the quality of a solution, and, as a consequence, the benefit of committing to a particular action. 

== Expected utility and Value of Information ==

Consider, again, a commuting problem. The agent must drive from city A to city B, and there are two roads connecting the cities. In Israel, for example, ther are two main routes between Tel Aviv and Jerusalem. The current belief of the
agent about the commute time through each of the routes looks like on the figure (think of two normal distributions with different means and variances).

Which route should the agent use? The answer depends on the utility of the travel time. For example,

   1) the agent wants to minimize the time on the road,
   2) the agent has to arrive by a particular time T,
   3) the agent wants to minimize the time on the road, but has
     to arrive by a particular time.

Each of the objectives can be formalized as a utility function from time to a utility measure, in some units.

   U_1(t) = -t
   U_2(t) = 1 if t<T, -1 
   U_3(t) = 1-t/T if t<T, -1 otherwise

The choice of a route according to the current belief must be performed according to the expected utility which is defined as the integral of the product of the utility function and the probability of the outcome for the continuous case, or as the corresponding sum in the discrete case.

EU=\int_{0}^{\infty} U(t)p(t)dt





