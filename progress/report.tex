\documentclass[]{article}
\usepackage{setspace,geometry}

\geometry{bindingoffset=0.4in,margin=1.2in}

\title{Progress Report Towards PhD Degree, Year Two\\
  Research Topic: Rational Metareasoning in Problem Solving Search}
\author{David Tolpin\\
        Department of Computer Science\\
        Ben-Gurion University of the Negev\\
        \\
        Under the supervision of Professor Solomon Eyal Shimony\\
        \\
        Author's Signature: \rule{10em}{1pt}\\
        Advisor's Signature: \rule{10em}{1pt}\\
        Chairman's Signature: \rule{10em}{1pt}\\
      \\}

\doublespacing

\begin{document}

\maketitle

\section{Research Progress}

During the second year of my Ph.D. studies I worked on rational
deployment of search heuristics (Section~\ref{sec:raticsp}), as well
as on improvements to Monte Carlo sampling algorithms for tree search
(Section~\ref{sec:doingbetter}). I also prepared revisions of two
journal papers  \cite{TolpinShimony.semimyopic},
\cite{TolpinShimony.raticomp} for publication later in 2001.

\subsection{Rational Deployment of CSP Heuristics}
\label{sec:raticsp}

We proposed a rational metareasoning approach to decide when and how to
deploy heuristics, using CSP backtracking search as a case study.  The
heuristics examined are various {\em solution count estimate}
heuristics for value ordering, which are expensive to compute, but can
significantly decrease the number of backtracks. These heuristics make
a good case study, as their overall utility, taking computational
overhead into account, is sometimes detrimental; and yet, by employing
these heuristics adaptively, it may still be possible to achieve an
overall runtime improvement, even in these pathological cases.
Following the metareasoning approach, the value of information (VOI)
of a heuristic is defined in terms of total search time saved, and the
heuristic is computed such that the expected net VOI is positive.

As a result of the research \cite{TolpinShimony.raticsp}, we suggested
a model for adaptive deployment of value ordering heuristics in
algorithms for constraint satisfaction problems. As a case study, the
model was applied to a value-ordering heuristic based on solution
count estimates, and a steady improvement in the overall algorithm
performance was achieved compared to {\em always} computing the
estimates, as well as to other simple deployment tactics.  The
experiments showed that for many problem instances the optimum
performance is achieved when solution counts are estimated only in a
relatively small number of search states.

The methods introduced in this paper can be extended in numerous
ways. First, generalization of the VOI to deploy different types of
heuristics for CSP, such as variable ordering heuristics, as well as
reasoning about deployment of more than one heuristic at a time, are
natural non-trivial extensions. Second, an explicit evaluation of the
quality of the distribution model is an interesting issue, coupled
with a better candidate model of the distribution.  Such distribution
models can also employ more disciplined statistical learning methods
in tandem, as suggested above. Finally, applying the methods suggested
in this paper to search in other domains can be attempted, especially
to heuristics for planning.  In particular, examining whether
the meta-reasoning scheme can improve reasoning over deployment of
heuristics based solely on learning methods is an interesting future
research issue.

The paper was presented at the 11th International Joint Conference on
Artificial Intelligence, Barcelona, Spain.

\subsection{Rational Monte Carlo Search}
\label{sec:doingbetter}

UCT, a state-of-the art algorithm for Monte Carlo tree sampling
(MCTS), is based on UCB, a sampling policy for the Multi-armed Bandit
Problem (MAB) that minimizes the accumulated regret. However, MCTS
differs from MAB in that only the final choice, rather than all arm
pulls, brings a reward, that is, the simple regret, as opposite to the
cumulative regret, must be minimized. This ongoing work
\cite{TolpinShimony.doingbetter} aims at applying meta-reasoning
techniques to MCTS, which is non-trivial.  We introduced policies for
multi-armed bandits with lower simple regret than UCB, and an
algorithm for MCTS which combines cumulative and simple regret
minimization and outperforms UCT. We also developed a sampling scheme
loosely based on a myopic version of perfect value of
information. Finite-time and asymptotic analysis of the policies was
provided, and the algorithms were compared empirically.
\cite{TolpinShimony.doingbetter}

\section{Publications}

\bibliographystyle{plain}
\bibliography{refs}

\end{document}
