While ideally programs, or agents, should {\em act rationally}
\cite{Russell.aima}, absolute rationality is not feasible. The
computational power of the agent approaching the problem must be taken
into account. Rational metareasoning \cite{Russell.right} is an
approach to building {\em bounded optimal} agents, agents which find a
solution that is optimal given their computational resources
\cite{Horvitz.reasoningabout}. The approach describes a method of
choosing {\em meta-level actions} and is based on notions of {\em
value of information} and {\em time cost}.

\subsection{Meta-level Actions}

The rational meta-reasoning framework aims at optimal
decision-making w.r.t. which computational operator should be applied
at every point in the search. In rational meta-reasoning, one can
define a meta-level decision problem over states of the belief space
with computational operators as meta-level actions, as a problem of
sequential decision-making under uncertainty. This meta-level problem
can be formalized as a (belief state) meta-level Markov decision
process (MDP) defined by a tuple $(Q, M, \pi, R)$ as follows. Each
state $Q_i \in Q$ of this MDP includes all current knowledge about the
search graph. For example, in a single-agent search problem, a state
is comprised of the known part of the search graph. including all
available heuristic estimates of nodes of this graph. For every
meta-level state $Q_i$ there is a corresponding base-level action
$A_i \in A$, such as transition to a neighbor node in the search graph, which
appears to be the best action given $Q_i$. The
meta-level actions $M_j \in M$ are all the potential computational
operators that are applicable to a state $Q_i$, such as calculating an
heuristic for a given node. The transition probabilities $\pi$ are
defined by the distributions over new knowledge that may be revealed
by applying each of the potential computational operators.  For
example, if we have a known distribution over heuristic values that we
expect a heuristic to yield, we can use the distribution to describe
our expectation of the potential (belief) state of the search after
(and if) the heuristic is computed at a search node.

The rewards $R$ are defined by the costs of the computational
operators, in terms of time, memory, etc. and by the utilities of the
base-level actions corresponding to the meta-level states.
At each step of the search algorithm the algorithm selects a
`best' base-level action $A_\alpha$ by solving the meta-level MDP.
The goal of meta-level actions is to refine the choice of $A_\alpha$.

\subsection{Value of Information}
\label{sec:ratimeta-voi}

A meta-level action affects the choice of the base-level action
$A_\alpha$ by changing the meta-level state.
The {\em value} of a meta-level action is measured by the
resulting increase in the utility of $A_\alpha$. Since neither the
outcomes of meta-level actions nor the true utility of $A_\alpha$ are
known in advance, a meta-level action is selected according to its
expected influence on the expected utility of $A_\alpha$.

$M_j$ affects the meta-level state and the effect of
a possible further meta-level action sequence $\mathbf{T}$. Thus, the expected utility of
$M_j$ is:
\begin{equation}
\label{eq:bg-limited-su}
\IE(U(M_j))=\sum_{\mathbf{T}}\Pr(\mathbf{T})\IE(U(A_{\alpha}^\mathbf{T},M_j\cdot\mathbf{T}))
\end{equation}
where $M_j\cdot\mathbf{T}$ denotes the meta-level action $M_j$
followed by possible further meta-level action sequence $\mathbf{T}$.
The {\em value of information} of a meta-level action $M_j$ is
the expected difference between the expected utility of $M_j$ and the
expected utility of the current $A_\alpha$.
\begin{equation}
\label{eq:bg-limited-nv}
V(M_j)=\IE(\IE(U(M_j))-\IE(U(A_\alpha)))
\end{equation}

While a perfectly rational agent would always choose the most valuable
computation sequence, an agent with only limited rationality makes
decisions based on an approximation of the utility.

\subsection{Benefit and Time Cost}
\label{sec:ratimeta-benefit-timecost}

The general dependence on the overall state complicates the analysis. Under
certain assumptions, it is possible to capture the dependence of utility on time in a
separate notion of {\em time cost} $C$. Then, the utility of an action $A_i$ taken
 after a meta-level action $M_j$ is the utility of $A_i$ taken now less the
cost of time for performing $M_j$:

\begin{equation}
\label{eq:bg-limited-iu}
U(A_i, M_j) = U(A_i) - C(A_i, M_j)
\end{equation}

It is customary to call the current utility of a future base-level
action its {\em intrinsic utility}. The separation into intrinsic
utility and time cost allows to estimate the utility of a base-level
action in a time-independent manner, and then refine the net utility
estimate according to the time pressure represented by $C$.

In many cases, the time cost of an internal action is independent of
the subsequently taken base-level action. When $C$ depends only on
$M_j$, (\ref{eq:bg-limited-nv}) can be rewritten with the cost and the
intrinsic value of information of a computation as separate terms.

\begin{eqnarray}
\label{eq:bg-limited-v=bc}
V(M_j)&=&\IE\left(\IE(U(A_\alpha^j, M_J))-\IE(U(A_\alpha))\right)\nonumber\\
     &=&\IE\left(\IE(U(A_\alpha^j))-\IE(U(A_\alpha))\right)-C(M_j)\\
     &=& \Lambda(M_j)-C(M_j)
\end{eqnarray}
where
\begin{equation}
\label{eq:bg-limited-benefit}
\Lambda(M_j)=\IE\left(\IE(U(A_\alpha^j))-\IE(U(A_\alpha))\right)
\end{equation}
denotes the intrinsic value of information, that is, the expected
difference between the intrinsic expected utilities of the new
and the old selected base-level action, computed after the meta-level
action was taken.

For any particular outcome of the meta-level action, one of the
following cases takes place:

\begin{enumerate}
\item the selected base-level action $A_\alpha$ stays the same;
  consequently, $\IE(U(A_\alpha^j))-\IE(U(A_\alpha))$ is zero;
\item a different base-level action $A_\alpha^j$ is selected, with
  {\bf higher} expected utility than the expected utility of
  $A_\alpha$ before the meta-level action was taken;
\item a different $A_\alpha^j$ is selected, and its expected
  utility is {\bf the same} as or {\bf lower} than the expected
  utility of $A_\alpha$ before the meta-level action was taken.
\end{enumerate}

In the last two cases, the difference is positive---although the
expected utility of the final choice can decrease, the latter
choice appears to be better than the earlier one due
to the updated knowledge about all actions. Thus, while the
net value of information can be either positive or negative depending on
the cost of the action, the intrinsic value of information is always
non-negative.

\subsection{Simplifying Assumptions}
\label{sec:ratimeta-assumptions}

Theoretically, as suggested by Russell and Wefald, optimally solving
the meta-level decision problem reveals an optimal policy to adopt at
every step of the search. However, the meta-level MDP is actually
harder to solve in general than the original search problem;
therefore, it makes no sense in practice to actually define and solve 
this MDP, at least not during search. Instead, Russell and Wefald
introduced a simplified model that is much easier to solve. The model
is based on the following \emph{simplifying
assumptions} \cite{Russell.right}: 
\begin{enumerate}
\item \textbf{Myopic:} all computations involved in estimation of the
value of information of meta-level actions assume that at most a single
meta-level action will be performed before a base-level action is
chosen.\footnote{In the original work \cite{Russell.right} the myopic
assumption is presented as two assumptions: ``single-step'' and
``meta-greedy''.}
\item All computational operators have a \textbf{known time cost}.
\item In addition, the \textbf{subtree independence} assumption can
be used to further simplify the model: every meta-level action
affects the utility estimate of a single base-level action. 
\end{enumerate}
Applied to certain search problems, this \emph{simplified
meta-reasoning approach}, even if not entirely justified, results in
improved search performance. However, even application of the
simplified approach is at least non-trivial in many cases, mostly due
to difficulties in obtaining beliefs, transition probabilities and
utility values, as well as due to high computational complexity of the
meta-reasoning level itself. On the other hand, none of the simplifying
assumptions hold in general, and in many cases their applicability is
limited. For example, with many computational operators the subtree
independence assumption is inappropriate --- a computation updates
the belief state, and along the new belief state utilities of several
base-level actions are updated. The myopic assumption is limited to
the cases when the value of information of a sequence of meta-level
actions is approximately submodular \cite{Guestrin.submodular}. When 
the intrinsic value of information is concave in the amount of
computation, the net value of information can be negative for any
single action, and the myopic assumption results in premature
termination of meta-level computation. 
